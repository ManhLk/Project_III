{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\MANHLK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\MANHLK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\MANHLK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\MANHLK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\MANHLK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\MANHLK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw_text's length:  235573\n",
      "Điện thoại di động Nokia 625H RM-943 CV VN YELLOW - A00013419 (Gồm thân máy, pin, sạc, sách, tai nghe, cáp kết nối), mới 100%\n",
      "Điện thoại di động Nokia\n"
     ]
    }
   ],
   "source": [
    "files = [os.path.join('train', f) for f in os.listdir('train')]\n",
    "sentence = []\n",
    "raw_text = ''\n",
    "for file in files:\n",
    "    with open(file, encoding='utf8') as file_in:\n",
    "        text = file_in.read()\n",
    "    raw_text += text\n",
    "print(\"Raw_text's length: \",len(raw_text))\n",
    "print(raw_text[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token: 64684\n",
      "Sampled token:  ['điện', 'thoại', 'di', 'động', 'nokia', '625h', 'rm', '-', '943', 'cv']\n"
     ]
    }
   ],
   "source": [
    "# Token\n",
    "special_char = ('&', '(', ')', '#', '-', '/', '.', ',', '%')\n",
    "for char in special_char:\n",
    "    raw_text = raw_text.replace(char, ' '+char+' ')\n",
    "    raw_text = raw_text.lower()\n",
    "token = raw_text.split()\n",
    "print('Number of token:', len(token))\n",
    "print('Sampled token: ', token[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab's length:  3626\n",
      "Sampled vocab:  ['qs16', 'a00021067', 'choc', '1a', '1700mah', 'kết', 'p16', '059v8f1', 'giắc', 'da']\n"
     ]
    }
   ],
   "source": [
    "# Buil vocab\n",
    "vocab = list(set(token))\n",
    "vocab_size = len(vocab) + 1\n",
    "print(\"Vocab's length: \", vocab_size)\n",
    "print(\"Sampled vocab: \", vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word to index\n",
    "word_index = {w: i+1 for i, w in enumerate(vocab)}\n",
    "word_index['<OOV>'] = 1\n",
    "\n",
    "# Index to word\n",
    "index_word = {i: w for w, i in word_index.items()}\n",
    "\n",
    "# Word to vector\n",
    "word_vector = [word_index[w] for w in token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X, y element\n",
    "sequences = []\n",
    "for i in range(1, len(word_vector)):\n",
    "    sequence = word_vector[i-1: i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,0], sequences[:, 1]\n",
    "\n",
    "\n",
    "# One hot vector output\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "# Split train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MANHLK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 64)             232064    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1, 256)            328704    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3626)              931882    \n",
      "=================================================================\n",
      "Total params: 2,017,962\n",
      "Trainable params: 2,017,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=1))\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics=['accuracy'])\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MANHLK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 46571 samples, validate on 5175 samples\n",
      "Epoch 1/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 4.4675 - accuracy: 0.2359 - val_loss: 3.2849 - val_accuracy: 0.4282\n",
      "Epoch 2/100\n",
      "46571/46571 [==============================] - 53s 1ms/step - loss: 2.8895 - accuracy: 0.4584 - val_loss: 2.7989 - val_accuracy: 0.4848\n",
      "Epoch 3/100\n",
      "46571/46571 [==============================] - 53s 1ms/step - loss: 2.4753 - accuracy: 0.5020 - val_loss: 2.6768 - val_accuracy: 0.4986\n",
      "Epoch 4/100\n",
      "46571/46571 [==============================] - 53s 1ms/step - loss: 2.2935 - accuracy: 0.5143 - val_loss: 2.6579 - val_accuracy: 0.5005\n",
      "Epoch 5/100\n",
      "46571/46571 [==============================] - 53s 1ms/step - loss: 2.1940 - accuracy: 0.5201 - val_loss: 2.6408 - val_accuracy: 0.5007\n",
      "Epoch 6/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 2.1267 - accuracy: 0.5224 - val_loss: 2.6443 - val_accuracy: 0.4945\n",
      "Epoch 7/100\n",
      "46571/46571 [==============================] - 57s 1ms/step - loss: 2.0816 - accuracy: 0.5238 - val_loss: 2.6444 - val_accuracy: 0.5057\n",
      "Epoch 8/100\n",
      "46571/46571 [==============================] - 59s 1ms/step - loss: 2.0471 - accuracy: 0.5264 - val_loss: 2.6564 - val_accuracy: 0.5040\n",
      "Epoch 9/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 2.0246 - accuracy: 0.5267 - val_loss: 2.6480 - val_accuracy: 0.5084\n",
      "Epoch 10/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 2.0026 - accuracy: 0.5289 - val_loss: 2.6477 - val_accuracy: 0.5061\n",
      "Epoch 11/100\n",
      "46571/46571 [==============================] - 53s 1ms/step - loss: 1.9854 - accuracy: 0.5304 - val_loss: 2.6543 - val_accuracy: 0.5057\n",
      "Epoch 12/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 1.9704 - accuracy: 0.5312 - val_loss: 2.6447 - val_accuracy: 0.5084\n",
      "Epoch 13/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 1.9593 - accuracy: 0.5300 - val_loss: 2.6519 - val_accuracy: 0.5088\n",
      "Epoch 14/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 1.9483 - accuracy: 0.5319 - val_loss: 2.6514 - val_accuracy: 0.5061\n",
      "Epoch 15/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 1.9406 - accuracy: 0.5317 - val_loss: 2.6532 - val_accuracy: 0.5053\n",
      "Epoch 16/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 1.9321 - accuracy: 0.5318 - val_loss: 2.6522 - val_accuracy: 0.5123\n",
      "Epoch 17/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 1.9240 - accuracy: 0.5344 - val_loss: 2.6539 - val_accuracy: 0.5049\n",
      "Epoch 18/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.9188 - accuracy: 0.5337 - val_loss: 2.6500 - val_accuracy: 0.5121\n",
      "Epoch 19/100\n",
      "46571/46571 [==============================] - 58s 1ms/step - loss: 1.9123 - accuracy: 0.5349 - val_loss: 2.6664 - val_accuracy: 0.5030\n",
      "Epoch 20/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.9075 - accuracy: 0.5348 - val_loss: 2.6603 - val_accuracy: 0.5115\n",
      "Epoch 21/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.9027 - accuracy: 0.5354 - val_loss: 2.6623 - val_accuracy: 0.5115\n",
      "Epoch 22/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8988 - accuracy: 0.5358 - val_loss: 2.6551 - val_accuracy: 0.5119\n",
      "Epoch 23/100\n",
      "46571/46571 [==============================] - 56s 1ms/step - loss: 1.8966 - accuracy: 0.5359 - val_loss: 2.6756 - val_accuracy: 0.5018\n",
      "Epoch 24/100\n",
      "46571/46571 [==============================] - 56s 1ms/step - loss: 1.8927 - accuracy: 0.5365 - val_loss: 2.6703 - val_accuracy: 0.5065\n",
      "Epoch 25/100\n",
      "46571/46571 [==============================] - 58s 1ms/step - loss: 1.8884 - accuracy: 0.5363 - val_loss: 2.6655 - val_accuracy: 0.5086\n",
      "Epoch 26/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8865 - accuracy: 0.5364 - val_loss: 2.6598 - val_accuracy: 0.5123\n",
      "Epoch 27/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8836 - accuracy: 0.5372 - val_loss: 2.6788 - val_accuracy: 0.5071\n",
      "Epoch 28/100\n",
      "46571/46571 [==============================] - 56s 1ms/step - loss: 1.8818 - accuracy: 0.5357 - val_loss: 2.6672 - val_accuracy: 0.5069\n",
      "Epoch 29/100\n",
      "46571/46571 [==============================] - 58s 1ms/step - loss: 1.8781 - accuracy: 0.5353 - val_loss: 2.6884 - val_accuracy: 0.5076\n",
      "Epoch 30/100\n",
      "46571/46571 [==============================] - 60s 1ms/step - loss: 1.8757 - accuracy: 0.5375 - val_loss: 2.6863 - val_accuracy: 0.5078\n",
      "Epoch 31/100\n",
      "46571/46571 [==============================] - 60s 1ms/step - loss: 1.8748 - accuracy: 0.5378 - val_loss: 2.6762 - val_accuracy: 0.5100\n",
      "Epoch 32/100\n",
      "46571/46571 [==============================] - 56s 1ms/step - loss: 1.8720 - accuracy: 0.5394 - val_loss: 2.6825 - val_accuracy: 0.4987\n",
      "Epoch 33/100\n",
      "46571/46571 [==============================] - 71s 2ms/step - loss: 1.8705 - accuracy: 0.5363 - val_loss: 2.6815 - val_accuracy: 0.5067\n",
      "Epoch 34/100\n",
      "46571/46571 [==============================] - 76s 2ms/step - loss: 1.8686 - accuracy: 0.5364 - val_loss: 2.6708 - val_accuracy: 0.5115\n",
      "Epoch 35/100\n",
      "46571/46571 [==============================] - 71s 2ms/step - loss: 1.8663 - accuracy: 0.5371 - val_loss: 2.6818 - val_accuracy: 0.5105\n",
      "Epoch 36/100\n",
      "46571/46571 [==============================] - 66s 1ms/step - loss: 1.8649 - accuracy: 0.5363 - val_loss: 2.6843 - val_accuracy: 0.5086\n",
      "Epoch 37/100\n",
      "46571/46571 [==============================] - 67s 1ms/step - loss: 1.8631 - accuracy: 0.5375 - val_loss: 2.6879 - val_accuracy: 0.5117\n",
      "Epoch 38/100\n",
      "46571/46571 [==============================] - 70s 2ms/step - loss: 1.8617 - accuracy: 0.5389 - val_loss: 2.6804 - val_accuracy: 0.5109\n",
      "Epoch 39/100\n",
      "46571/46571 [==============================] - 78s 2ms/step - loss: 1.8612 - accuracy: 0.5358 - val_loss: 2.6762 - val_accuracy: 0.5150\n",
      "Epoch 40/100\n",
      "46571/46571 [==============================] - 65s 1ms/step - loss: 1.8589 - accuracy: 0.5381 - val_loss: 2.6889 - val_accuracy: 0.5111\n",
      "Epoch 41/100\n",
      "46571/46571 [==============================] - 67s 1ms/step - loss: 1.8576 - accuracy: 0.5388 - val_loss: 2.6813 - val_accuracy: 0.5109\n",
      "Epoch 42/100\n",
      "46571/46571 [==============================] - 67s 1ms/step - loss: 1.8552 - accuracy: 0.5397 - val_loss: 2.6919 - val_accuracy: 0.5109\n",
      "Epoch 43/100\n",
      "46571/46571 [==============================] - 61s 1ms/step - loss: 1.8541 - accuracy: 0.5396 - val_loss: 2.6877 - val_accuracy: 0.5136\n",
      "Epoch 44/100\n",
      "46571/46571 [==============================] - 74s 2ms/step - loss: 1.8535 - accuracy: 0.5383 - val_loss: 2.6835 - val_accuracy: 0.5136\n",
      "Epoch 45/100\n",
      "46571/46571 [==============================] - 70s 1ms/step - loss: 1.8515 - accuracy: 0.5394 - val_loss: 2.7007 - val_accuracy: 0.5088\n",
      "Epoch 46/100\n",
      "46571/46571 [==============================] - 72s 2ms/step - loss: 1.8508 - accuracy: 0.5387 - val_loss: 2.6882 - val_accuracy: 0.5090\n",
      "Epoch 47/100\n",
      "46571/46571 [==============================] - 77s 2ms/step - loss: 1.8493 - accuracy: 0.5390 - val_loss: 2.6969 - val_accuracy: 0.5090\n",
      "Epoch 48/100\n",
      "46571/46571 [==============================] - 87s 2ms/step - loss: 1.8485 - accuracy: 0.5396 - val_loss: 2.6972 - val_accuracy: 0.5109\n",
      "Epoch 49/100\n",
      "46571/46571 [==============================] - 87s 2ms/step - loss: 1.8467 - accuracy: 0.5400 - val_loss: 2.6945 - val_accuracy: 0.5082\n",
      "Epoch 50/100\n",
      "46571/46571 [==============================] - 69s 1ms/step - loss: 1.8463 - accuracy: 0.5404 - val_loss: 2.6966 - val_accuracy: 0.5107\n",
      "Epoch 51/100\n",
      "46571/46571 [==============================] - 70s 2ms/step - loss: 1.8445 - accuracy: 0.5394 - val_loss: 2.7082 - val_accuracy: 0.5067\n",
      "Epoch 52/100\n",
      "46571/46571 [==============================] - 64s 1ms/step - loss: 1.8441 - accuracy: 0.5407 - val_loss: 2.6919 - val_accuracy: 0.5157\n",
      "Epoch 53/100\n",
      "46571/46571 [==============================] - 63s 1ms/step - loss: 1.8425 - accuracy: 0.5398 - val_loss: 2.6883 - val_accuracy: 0.5129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "46571/46571 [==============================] - 67s 1ms/step - loss: 1.8436 - accuracy: 0.5403 - val_loss: 2.6992 - val_accuracy: 0.5096\n",
      "Epoch 55/100\n",
      "46571/46571 [==============================] - 64s 1ms/step - loss: 1.8417 - accuracy: 0.5385 - val_loss: 2.6955 - val_accuracy: 0.5094\n",
      "Epoch 56/100\n",
      "46571/46571 [==============================] - 68s 1ms/step - loss: 1.8410 - accuracy: 0.5397 - val_loss: 2.6865 - val_accuracy: 0.5088\n",
      "Epoch 57/100\n",
      "46571/46571 [==============================] - 60s 1ms/step - loss: 1.8388 - accuracy: 0.5404 - val_loss: 2.6992 - val_accuracy: 0.5109\n",
      "Epoch 58/100\n",
      "46571/46571 [==============================] - 53s 1ms/step - loss: 1.8396 - accuracy: 0.5395 - val_loss: 2.7014 - val_accuracy: 0.5103\n",
      "Epoch 59/100\n",
      "46571/46571 [==============================] - 64s 1ms/step - loss: 1.8374 - accuracy: 0.5409 - val_loss: 2.6965 - val_accuracy: 0.5082\n",
      "Epoch 60/100\n",
      "46571/46571 [==============================] - 62s 1ms/step - loss: 1.8374 - accuracy: 0.5403 - val_loss: 2.7032 - val_accuracy: 0.5090\n",
      "Epoch 61/100\n",
      "46571/46571 [==============================] - 63s 1ms/step - loss: 1.8356 - accuracy: 0.5415 - val_loss: 2.7039 - val_accuracy: 0.5146\n",
      "Epoch 62/100\n",
      "46571/46571 [==============================] - 57s 1ms/step - loss: 1.8351 - accuracy: 0.5409 - val_loss: 2.6977 - val_accuracy: 0.5157\n",
      "Epoch 63/100\n",
      "46571/46571 [==============================] - 61s 1ms/step - loss: 1.8344 - accuracy: 0.5412 - val_loss: 2.7026 - val_accuracy: 0.5103\n",
      "Epoch 64/100\n",
      "46571/46571 [==============================] - 60s 1ms/step - loss: 1.8337 - accuracy: 0.5418 - val_loss: 2.6968 - val_accuracy: 0.5154\n",
      "Epoch 65/100\n",
      "46571/46571 [==============================] - 57s 1ms/step - loss: 1.8324 - accuracy: 0.5403 - val_loss: 2.7086 - val_accuracy: 0.5123\n",
      "Epoch 66/100\n",
      "46571/46571 [==============================] - 60s 1ms/step - loss: 1.8324 - accuracy: 0.5413 - val_loss: 2.6976 - val_accuracy: 0.5157\n",
      "Epoch 67/100\n",
      "46571/46571 [==============================] - 62s 1ms/step - loss: 1.8318 - accuracy: 0.5421 - val_loss: 2.6975 - val_accuracy: 0.5117\n",
      "Epoch 68/100\n",
      "46571/46571 [==============================] - 67s 1ms/step - loss: 1.8321 - accuracy: 0.5411 - val_loss: 2.6990 - val_accuracy: 0.5165\n",
      "Epoch 69/100\n",
      "46571/46571 [==============================] - 82s 2ms/step - loss: 1.8294 - accuracy: 0.5408 - val_loss: 2.7111 - val_accuracy: 0.5140\n",
      "Epoch 70/100\n",
      "46571/46571 [==============================] - 64s 1ms/step - loss: 1.8290 - accuracy: 0.5418 - val_loss: 2.7087 - val_accuracy: 0.5107\n",
      "Epoch 71/100\n",
      "46571/46571 [==============================] - 63s 1ms/step - loss: 1.8290 - accuracy: 0.5427 - val_loss: 2.6991 - val_accuracy: 0.5144\n",
      "Epoch 72/100\n",
      "46571/46571 [==============================] - 64s 1ms/step - loss: 1.8273 - accuracy: 0.5427 - val_loss: 2.7079 - val_accuracy: 0.5138\n",
      "Epoch 73/100\n",
      "46571/46571 [==============================] - 71s 2ms/step - loss: 1.8263 - accuracy: 0.5425 - val_loss: 2.7105 - val_accuracy: 0.5123\n",
      "Epoch 74/100\n",
      "46571/46571 [==============================] - 73s 2ms/step - loss: 1.8268 - accuracy: 0.5411 - val_loss: 2.7126 - val_accuracy: 0.5101\n",
      "Epoch 75/100\n",
      "46571/46571 [==============================] - 77s 2ms/step - loss: 1.8250 - accuracy: 0.5422 - val_loss: 2.7016 - val_accuracy: 0.5138\n",
      "Epoch 76/100\n",
      "46571/46571 [==============================] - 79s 2ms/step - loss: 1.8252 - accuracy: 0.5423 - val_loss: 2.7114 - val_accuracy: 0.5150\n",
      "Epoch 77/100\n",
      "46571/46571 [==============================] - 73s 2ms/step - loss: 1.8241 - accuracy: 0.5434 - val_loss: 2.7116 - val_accuracy: 0.5101\n",
      "Epoch 78/100\n",
      "46571/46571 [==============================] - 69s 1ms/step - loss: 1.8239 - accuracy: 0.5426 - val_loss: 2.7147 - val_accuracy: 0.5125\n",
      "Epoch 79/100\n",
      "46571/46571 [==============================] - 81s 2ms/step - loss: 1.8234 - accuracy: 0.5427 - val_loss: 2.7092 - val_accuracy: 0.5119\n",
      "Epoch 80/100\n",
      "46571/46571 [==============================] - 72s 2ms/step - loss: 1.8216 - accuracy: 0.5431 - val_loss: 2.7139 - val_accuracy: 0.5117\n",
      "Epoch 81/100\n",
      "46571/46571 [==============================] - 62s 1ms/step - loss: 1.8218 - accuracy: 0.5438 - val_loss: 2.7209 - val_accuracy: 0.5121\n",
      "Epoch 82/100\n",
      "46571/46571 [==============================] - 64s 1ms/step - loss: 1.8216 - accuracy: 0.5430 - val_loss: 2.7203 - val_accuracy: 0.5109\n",
      "Epoch 83/100\n",
      "46571/46571 [==============================] - 61s 1ms/step - loss: 1.8215 - accuracy: 0.5437 - val_loss: 2.7130 - val_accuracy: 0.5123\n",
      "Epoch 84/100\n",
      "46571/46571 [==============================] - 56s 1ms/step - loss: 1.8201 - accuracy: 0.5429 - val_loss: 2.7145 - val_accuracy: 0.5146\n",
      "Epoch 85/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8206 - accuracy: 0.5431 - val_loss: 2.7088 - val_accuracy: 0.5148\n",
      "Epoch 86/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8193 - accuracy: 0.5439 - val_loss: 2.7131 - val_accuracy: 0.5076\n",
      "Epoch 87/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 1.8194 - accuracy: 0.5443 - val_loss: 2.7132 - val_accuracy: 0.5109\n",
      "Epoch 88/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8188 - accuracy: 0.5436 - val_loss: 2.7172 - val_accuracy: 0.5129\n",
      "Epoch 89/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8176 - accuracy: 0.5444 - val_loss: 2.7051 - val_accuracy: 0.5134\n",
      "Epoch 90/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8181 - accuracy: 0.5431 - val_loss: 2.7178 - val_accuracy: 0.5127\n",
      "Epoch 91/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8166 - accuracy: 0.5428 - val_loss: 2.7196 - val_accuracy: 0.5127\n",
      "Epoch 92/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8157 - accuracy: 0.5458 - val_loss: 2.7167 - val_accuracy: 0.5148\n",
      "Epoch 93/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8168 - accuracy: 0.5434 - val_loss: 2.7168 - val_accuracy: 0.5165\n",
      "Epoch 94/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8165 - accuracy: 0.5451 - val_loss: 2.7181 - val_accuracy: 0.5057\n",
      "Epoch 95/100\n",
      "46571/46571 [==============================] - 55s 1ms/step - loss: 1.8165 - accuracy: 0.5441 - val_loss: 2.7182 - val_accuracy: 0.5109\n",
      "Epoch 96/100\n",
      "46571/46571 [==============================] - 57s 1ms/step - loss: 1.8150 - accuracy: 0.5441 - val_loss: 2.7178 - val_accuracy: 0.5121\n",
      "Epoch 97/100\n",
      "46571/46571 [==============================] - 61s 1ms/step - loss: 1.8143 - accuracy: 0.5445 - val_loss: 2.7185 - val_accuracy: 0.5107\n",
      "Epoch 98/100\n",
      "46571/46571 [==============================] - 65s 1ms/step - loss: 1.8135 - accuracy: 0.5436 - val_loss: 2.7285 - val_accuracy: 0.5072\n",
      "Epoch 99/100\n",
      "46571/46571 [==============================] - 61s 1ms/step - loss: 1.8139 - accuracy: 0.5452 - val_loss: 2.7246 - val_accuracy: 0.5115\n",
      "Epoch 100/100\n",
      "46571/46571 [==============================] - 54s 1ms/step - loss: 1.8132 - accuracy: 0.5439 - val_loss: 2.7326 - val_accuracy: 0.5121\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "epochs  = 100\n",
    "\n",
    "if not os.path.exists(\"language_model.hdf5\"):\n",
    "    checkpoint = ModelCheckpoint(filepath = 'language_model.hdf5', save_best_only = True, monitor='val_loss')\n",
    "    history = model.fit(X_train, y_train, epochs=epochs,\n",
    "                        validation_split=0.1, callbacks=[checkpoint])\n",
    "else:\n",
    "    model.load_weights(\"language_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
